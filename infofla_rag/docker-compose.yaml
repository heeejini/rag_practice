services:
  qdrant:
    image: qdrant/qdrant:latest
    container_name: infofla-rag-qdrant
    ports:
      - "6335:6333"
    volumes:
      - qdrant_data:/qdrant/storage
    restart: unless-stopped

  vllm:
    image: vllm/vllm-openai:v0.11.0
    container_name: infofla-rag-vllm
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ["3"]
              capabilities: ["gpu"]
    environment:
      - NVIDIA_VISIBLE_DEVICES=3
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - HF_TOKEN=${HF_TOKEN}
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
    command: >
      --model K-intelligence/Midm-2.0-Base-Instruct
      --host 0.0.0.0
      --port 8000
      --gpu-memory-utilization 0.85
      --enforce-eager
    ports:
      - "8001:8000"
    restart: unless-stopped

  app:
    build: .
    container_name: infofla-rag-app
    depends_on:
      - qdrant
      - vllm
    extra_hosts:
      - "host.docker.internal:host-gateway"
    environment:
      PYTHONPATH: /app 
      QDRANT_HOST: qdrant
      QDRANT_PORT: 6333
      VLLM_API_BASE: http://vllm:8000
      INDEX_SRC_DIR: /app/data/news_articles_preprocessing
    volumes:
      - ./data/news_articles_preprocessing:/app/data/news_articles_preprocessing:ro
    ports:
      - "9000:9000"
    restart: unless-stopped

volumes:
  qdrant_data:
